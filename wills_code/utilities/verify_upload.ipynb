{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime \n",
    "#from brtdevkit.ml import Workflow\n",
    "import pytz\n",
    "\n",
    "#from brtdevkit.ml import Workflow\n",
    "from brtdevkit.ml.core import KubeflowPipeline, KubeflowPipelineRun\n",
    "\n",
    "image_ingestion_pipeline = None\n",
    "isp_target_rom = '07090103'\n",
    "\n",
    "utc = pytz.UTC\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check To See if All Files Have Been uploaded from a SSD\n",
    "\n",
    "import time\n",
    "s3 = boto3.client('s3')\n",
    "image_ingestion_experiment_name = 'shasta_image_ingest'\n",
    "image_ingestion_pipeline_name = 'Shasta Image Data Processor'\n",
    "image_ingestion_pipeline = None\n",
    "\n",
    "def get_logfile_directories(bucket, prefix, just_fsize =False):\n",
    "    \"\"\"\n",
    "    Finds S3 directories with flatbuffer logfiles in them.\n",
    "    :param bucket: S3 bucket to search in\n",
    "    :param prefix: the prefix (pathname) underneath the bucket to be searched\n",
    "    \"\"\"\n",
    "\n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    dir_list = []\n",
    "    fsize = 0\n",
    "    kwargs = {'Bucket': bucket, 'Prefix': prefix}\n",
    "    while True:\n",
    "        resp = s3.list_objects_v2(MaxKeys =10000, **kwargs)\n",
    "        \n",
    "        # This bit of code handles pagination in the boto library.\n",
    "        if 'Contents' in resp:\n",
    "            for entry in resp['Contents']:\n",
    "                if entry['Key'].endswith('.bfbs'):\n",
    "                    dir_list.append(os.path.dirname(entry['Key']))\n",
    "                    fsize += entry['Size']\n",
    "            try:\n",
    "                kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "            except KeyError:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    if just_fsize == True:\n",
    "        return fsize\n",
    "    else:\n",
    "        return dir_list, fsize\n",
    "    \n",
    "def get_SSD_file_size(start_path):\n",
    "    \"\"\"\n",
    "    Given a path to an SSD, get the size of the files. \n",
    "    \"\"\"\n",
    "    \n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "\n",
    "    return total_size\n",
    "\n",
    "def check_upload_complete(bucket, prefix, SSD_path, vpu):\n",
    "    \"\"\"\n",
    "    Given a bucket and prefix in S3, as well as the path to the SSD from my laptop\n",
    "    Verify that eveything on the SSD was in fact uploaded to S3. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve all log file names on the SSD\n",
    "    SSD_dirs = []\n",
    "    for f in os.listdir(SSD_path):\n",
    "        li = os.listdir(SSD_path + f)\n",
    "        SSD_dirs.append(prefix  + vpu +'/full/' +f)\n",
    "\n",
    "    # Retrieve all logfile names in S3\n",
    "    s3_dirs, fsize = get_logfile_directories(bucket = bucket, prefix = prefix)\n",
    "    s3_dirs = set(s3_dirs)\n",
    "\n",
    "    print(f'There are {len(SSD_dirs)} logfiles on the SSD and {len(s3_dirs)} in s3.')\n",
    "    \n",
    "    # Check SSD against S3\n",
    "    s3_fsize = 0\n",
    "    for n in np.unique(SSD_dirs):\n",
    "        if n in s3_dirs:\n",
    "            fsize = get_logfile_directories(bucket = bucket, prefix = n, just_fsize=True)\n",
    "            s3_fsize = s3_fsize + fsize\n",
    "    \n",
    "    # Compare filesizes\n",
    "    s3_size = s3_fsize/1000000\n",
    "    SSD_fsize = get_SSD_file_size(SSD_path)/1000000\n",
    "    equal_fsize = abs(SSD_fsize - s3_size) < 1000\n",
    "    \n",
    "    # Compare logs\n",
    "    all_logs_in_s3 = set(SSD_dirs).discard(set(s3_dirs)) == None\n",
    "    \n",
    "    if all_logs_in_s3 and equal_fsize:\n",
    "        print(f'File size of {s3_size:.0f} MB confirmed')\n",
    "        print('All files on drive accounted for in S3.')\n",
    "        #return dirs, s3_size\n",
    "    elif all_logs_in_s3:\n",
    "        print(f'Could not verify filesize. There is a discepancy of {SSD_fsize - s3_size} out of {SSD_fsize}. Check paths')\n",
    "    \n",
    "    else:\n",
    "        print('Upload looks strange. Verify path.')\n",
    "        return set(SSD_dirs).discard(set(s3_dirs)), s3_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 21 logfiles on the SSD and 18158 in s3.\n",
      "File size of 197416 MB confirmed\n",
      "All files on drive accounted for in S3.\n"
     ]
    }
   ],
   "source": [
    "vpu = 'vpu0-4a' # change this if needed, for instance if verifying a machine upload on vpu0-4a\n",
    "bucket = 'brt-dcm-data'\n",
    "prefix = 'db1/'\n",
    "SSD_path ='/media/williamroberts/SSD298/db1-sysbox1/'+ vpu + '/full/'\n",
    "\n",
    "check_upload_complete(bucket , prefix , SSD_path , vpu = vpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 logfiles on the SSD and 4320 in s3.\n",
      "File size of 11911 MB confirmed\n",
      "All files on drive accounted for in S3.\n"
     ]
    }
   ],
   "source": [
    "vpu = 'vpu0-4a' # change this if needed, for instance if verifying a machine upload on vpu0-4a\n",
    "bucket = 'brt-dcm-data'\n",
    "prefix = 'db4/'\n",
    "SSD_path ='/media/williamroberts/SSD273/db4-sysbox1/'+ vpu + '/full/'\n",
    "\n",
    "check_upload_complete(bucket , prefix , SSD_path , vpu = vpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 219 logfiles on the SSD and 1021 in s3.\n",
      "File size of 1727225 MB confirmed\n",
      "All files on drive accounted for in S3.\n"
     ]
    }
   ],
   "source": [
    "vpu = 'vpu0-0a' # change this if needed, for instance if verifying a machine upload on vpu0-4a\n",
    "bucket = 'brt-dcm-data'\n",
    "prefix = 'dcm12/'\n",
    "SSD_path ='/media/williamroberts/SSD209/dcm12-sysbox1/'+ vpu + '/full/'\n",
    "\n",
    "check_upload_complete(bucket , prefix , SSD_path , vpu = vpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
